{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer, AutoModel, BertModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from py_files.models import BertEncoder, ResnetPreTrained, ImageEncoder\n",
    "from py_files.datasets import WSIBatchedDataset, GetRepsDataset\n",
    "\n",
    "import faiss\n",
    "\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "Device name: Tesla K80\n",
      "Device name: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "def check_cuda():\n",
    "    if torch.cuda.is_available():       \n",
    "        device = torch.device(\"cuda\")\n",
    "        device_count = torch.cuda.device_count()\n",
    "        print(f'There are {device_count} GPU(s) available.')\n",
    "        for i in range(device_count):\n",
    "            print('Device name:', torch.cuda.get_device_name(i))\n",
    "        return device\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "        return device\n",
    "    \n",
    "device=check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch_paths</th>\n",
       "      <th>pid</th>\n",
       "      <th>svs_paths</th>\n",
       "      <th>dtype</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         patch_paths             pid  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "\n",
       "                                           svs_paths  dtype  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "\n",
       "                                               notes  \n",
       "0  2 pieces ~9.5x7 mm; 1 broken apart; good morph...  \n",
       "1  2 pieces ~9.5x7 mm; 1 broken apart; good morph...  \n",
       "2  2 pieces ~9.5x7 mm; 1 broken apart; good morph...  \n",
       "3  2 pieces ~9.5x7 mm; 1 broken apart; good morph...  \n",
       "4  2 pieces ~9.5x7 mm; 1 broken apart; good morph...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train global KMeans clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pids = df.pid.unique()\n",
    "len(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_percent_cluster=0.1\n",
    "n_pids_cluster = int(pid_percent_cluster*len(pids))\n",
    "n_pids_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7677"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pids_cluster = np.random.choice(pids, size=n_pids_cluster)\n",
    "\n",
    "normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    normalize,\n",
    "        ])\n",
    "\n",
    "cluster_dataset = GetRepsDataset(df, pids_cluster, transform)\n",
    "cluster_loader = torch.utils.data.DataLoader(cluster_dataset,batch_size=64, shuffle=True, \\\n",
    "                                             num_workers=1, pin_memory=True)\n",
    "len(cluster_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.279168\n"
     ]
    }
   ],
   "source": [
    "base_image_model = ResnetPreTrained()\n",
    "image_encoder = nn.DataParallel(ImageEncoder(base_image_model))\n",
    "image_encoder.to(device)\n",
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global_cluster_model(encoder, dataloader, device, ncentroids=num_cluster):\n",
    "    \n",
    "    print(\"Getting patch representations...\")\n",
    "    \n",
    "    rep_list = []\n",
    "    path_list = []\n",
    "    \n",
    "    encoder.eval()\n",
    "    \n",
    "    for img, path in tqdm(dataloader):\n",
    "\n",
    "        img=img.to(device)    \n",
    "\n",
    "        in_batch_size = img.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reps = encoder(img)\n",
    "        rep_list.append(reps.detach().detach().cpu().numpy().reshape(in_batch_size, -1))\n",
    "        path_list += path\n",
    "        \n",
    "        # clean up\n",
    "        del img\n",
    "        del reps\n",
    "    \n",
    "    print(\"\\nTraining KMeans model...\")\n",
    "    \n",
    "    X = np.concatenate(rep_list)\n",
    "    X = np.ascontiguousarray(X)\n",
    "    X = X.astype('float32')\n",
    "    \n",
    "    ncentroids = ncentroids\n",
    "    niter = 300\n",
    "    verbose = False\n",
    "    d = X.shape[1]\n",
    "    kmeans = faiss.Kmeans(d, ncentroids, niter=niter, verbose=verbose, nredo=20)\n",
    "    kmeans.train(X)\n",
    "    \n",
    "    print(\"\\nFinished training KMeans model...\")\n",
    "    \n",
    "    # clean up\n",
    "    del encoder\n",
    "    del dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting patch representations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e7e3531ac34765b3a13f1ffa6fec49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "Training KMeans model...\n",
      "\n",
      "Finished training KMeans model...\n"
     ]
    }
   ],
   "source": [
    "kmeans = train_global_cluster_model(image_encoder, cluster_loader, device, ncentroids=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.279168\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://forums.fast.ai/t/gpu-memory-not-being-freed-after-training-is-over/10265/8?u=cedric\n",
    "# def pretty_size(size):\n",
    "# \t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "# \tassert(isinstance(size, torch.Size))\n",
    "# \treturn \" × \".join(map(str, size))\n",
    "\n",
    "# def dump_tensors(gpu_only=True):\n",
    "# \t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "# \timport gc\n",
    "# \ttotal_size = 0\n",
    "# \tfor obj in gc.get_objects():\n",
    "# \t\ttry:\n",
    "# \t\t\tif torch.is_tensor(obj):\n",
    "# \t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "# \t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "# \t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "# \t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "# \t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "# \t\t\t\t\ttotal_size += obj.numel()\n",
    "# \t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "# \t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "# \t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "# \t\t\t\t\ttotal_size += obj.data.numel()\n",
    "# \t\texcept Exception as e:\n",
    "# \t\t\tpass        \n",
    "# \tprint(\"Total size:\", total_size*1e-6)\n",
    "\n",
    "# dump_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster all patches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69757"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcluster_dataset = GetRepsDataset(df, pids, transform)\n",
    "gcluster_loader = torch.utils.data.DataLoader(gcluster_dataset,batch_size=128, shuffle=False, \\\n",
    "                                             num_workers=1, pin_memory=True)\n",
    "len(gcluster_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133.33862399999998\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_all_patches(encoder, kmeans, dataloader, device, ncentroids=8):\n",
    "    \n",
    "    print(\"Clustering all patches...\")\n",
    "    \n",
    "    encoder.eval()\n",
    "    \n",
    "    path_list, rep_list = [], []\n",
    "    \n",
    "    for img, path in tqdm(dataloader):\n",
    "        img=img.to(device)\n",
    "        \n",
    "        in_batch_size = img.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reps = image_encoder(img)\n",
    "        rep_list.append(reps.detach().detach().cpu().numpy().reshape(in_batch_size, -1))\n",
    "        path_list += path\n",
    "        \n",
    "        # clean up\n",
    "        del img\n",
    "        del reps\n",
    "        \n",
    "    X = np.concatenate(rep_list)\n",
    "    X = np.ascontiguousarray(X)\n",
    "    X = X.astype('float32')\n",
    "    \n",
    "    D, I = kmeans.index.search(X, 1)\n",
    "    \n",
    "    df = pd.DataFrame(path_list, columns=['patch_paths'])\n",
    "    df['cluster_assignment'] = I\n",
    "    \n",
    "    print(\"\\nFinished clustering all patches...\")\n",
    "    \n",
    "    # clean up\n",
    "    del encoder\n",
    "    # del dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering all patches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e5d181f9a6427a8c7b3e072a9fc999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "Finished clustering all patches...\n"
     ]
    }
   ],
   "source": [
    "cluster_df = cluster_all_patches(image_encoder, kmeans, gcluster_loader, device, ncentroids=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133.33862399999998\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered = df.merge(cluster_df, on='patch_paths')\n",
    "\n",
    "df_clustered.to_csv('../df_clustered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538.26816\n"
     ]
    }
   ],
   "source": [
    "# del image_encoder\n",
    "# del cluster_loader\n",
    "# del cluster_dataset\n",
    "# del gcluster_loader\n",
    "# del gcluster_dataset\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoint - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch_paths</th>\n",
       "      <th>pid</th>\n",
       "      <th>svs_paths</th>\n",
       "      <th>dtype</th>\n",
       "      <th>notes</th>\n",
       "      <th>cluster_assignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         patch_paths             pid  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "\n",
       "                                           svs_paths  dtype  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "\n",
       "                                               notes  cluster_assignment  \n",
       "0  2 pieces ~9.5x7 mm; 1 broken apart; good morph...                   7  \n",
       "1  2 pieces ~9.5x7 mm; 1 broken apart; good morph...                   0  \n",
       "2  2 pieces ~9.5x7 mm; 1 broken apart; good morph...                   0  \n",
       "3  2 pieces ~9.5x7 mm; 1 broken apart; good morph...                   0  \n",
       "4  2 pieces ~9.5x7 mm; 1 broken apart; good morph...                   0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../df_clustered.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    166\n",
       "7     16\n",
       "1     15\n",
       "4     11\n",
       "Name: cluster_assignment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.pid=='GTEX-R55E-1726'].cluster_assignment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_loss(cnn_code, rnn_code, eps=1e-8, temp3=10.0):\n",
    "\n",
    "    batch_size = cnn_code.shape[0]\n",
    "    labels = Variable(torch.LongTensor(range(batch_size))).to(cnn_code.device)\n",
    "\n",
    "    if cnn_code.dim() == 2 :\n",
    "        cnn_code = cnn_code.unsqueeze(0)\n",
    "        rnn_code = rnn_code.unsqueeze(0)\n",
    "        \n",
    "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
    "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
    "\n",
    "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1,2))\n",
    "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
    "    scores0 = scores0 / norm0.clamp(min=eps) * temp3\n",
    "    \n",
    "    # --> batch_size x batch_size\n",
    "    if scores0.shape[0]!=1:\n",
    "        scores0 = scores0.squeeze()\n",
    "    else:\n",
    "        scores0 = scores0.squeeze(0)\n",
    "    \n",
    "\n",
    "    scores1 = scores0.transpose(0, 1)\n",
    "    loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
    "    loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
    "    return loss0, loss1\n",
    "\n",
    "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n",
    "\n",
    "def attention_fn(query, context, temp1):\n",
    "    \"\"\"\n",
    "    query: batch x ndf x queryL\n",
    "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
    "    mask: batch_size x sourceL\n",
    "    \"\"\"\n",
    "    batch_size, queryL = query.size(0), query.size(2)\n",
    "    ih, iw = context.size(2), context.size(3)\n",
    "    sourceL = ih * iw\n",
    "\n",
    "    # --> batch x sourceL x ndf\n",
    "    context = context.view(batch_size, -1, sourceL)\n",
    "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
    "\n",
    "    # Get attention\n",
    "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
    "    # -->batch x sourceL x queryL\n",
    "    attn = torch.bmm(contextT, query)\n",
    "    # --> batch*sourceL x queryL\n",
    "    attn = attn.view(batch_size * sourceL, queryL)\n",
    "    attn = nn.Softmax(dim=-1)(attn)\n",
    "\n",
    "    # --> batch x sourceL x queryL\n",
    "    attn = attn.view(batch_size, sourceL, queryL)\n",
    "    # --> batch*queryL x sourceL\n",
    "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "    attn = attn.view(batch_size * queryL, sourceL)\n",
    "\n",
    "    attn = attn * temp1\n",
    "    attn = nn.Softmax(dim=-1)(attn)\n",
    "    attn = attn.view(batch_size, queryL, sourceL)\n",
    "    # --> batch x sourceL x queryL\n",
    "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
    "    # --> batch x ndf x queryL\n",
    "    weightedContext = torch.bmm(context, attnT)\n",
    "\n",
    "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "def local_loss(\n",
    "    img_features, words_emb, cap_lens, temp1=4.0, temp2=5.0, temp3=10.0, agg=\"sum\"\n",
    "):\n",
    "\n",
    "    batch_size = img_features.shape[0]\n",
    "\n",
    "    att_maps = []\n",
    "    similarities = []\n",
    "    # cap_lens = cap_lens.data.tolist()\n",
    "    for i in range(words_emb.shape[0]):\n",
    "\n",
    "        # Get the i-th text description\n",
    "        words_num = cap_lens[i]  # 25\n",
    "        # TODO: remove [SEP]\n",
    "        # word = words_emb[i, :, 1:words_num+1].unsqueeze(0).contiguous()    # [1, 768, 25]\n",
    "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()  # [1, 768, 25]\n",
    "        word = word.repeat(batch_size, 1, 1)  # [48, 768, 25]\n",
    "        context = img_features  # [48, 768, 19, 19]\n",
    "\n",
    "        weiContext, attn = attention_fn(\n",
    "            word, context, temp1\n",
    "        )  # [48, 768, 25], [48, 25, 19, 19]\n",
    "\n",
    "        att_maps.append(\n",
    "            attn[i].unsqueeze(0).contiguous()\n",
    "        )  # add attention for curr index  [25, 19, 19]\n",
    "        word = word.transpose(1, 2).contiguous()  # [48, 25, 768]\n",
    "        weiContext = weiContext.transpose(1, 2).contiguous()  # [48, 25, 768]\n",
    "\n",
    "        word = word.view(batch_size * words_num, -1)  # [1200, 768]\n",
    "        weiContext = weiContext.view(batch_size * words_num, -1)  # [1200, 768]\n",
    "\n",
    "        row_sim = cosine_similarity(word, weiContext)\n",
    "        row_sim = row_sim.view(batch_size, words_num)  # [48, 25]\n",
    "\n",
    "        row_sim.mul_(temp2).exp_()\n",
    "        if agg == \"sum\":\n",
    "            row_sim = row_sim.sum(dim=1, keepdim=True)  # [48, 1]\n",
    "        else:\n",
    "            row_sim = row_sim.mean(dim=1, keepdim=True)  # [48, 1]\n",
    "        row_sim = torch.log(row_sim)\n",
    "\n",
    "        similarities.append(row_sim)\n",
    "\n",
    "    similarities = torch.cat(similarities, 1)  #\n",
    "    similarities = similarities * temp3\n",
    "    similarities1 = similarities.transpose(0, 1)  # [48, 48]\n",
    "\n",
    "    labels = Variable(torch.LongTensor(range(batch_size))).to(similarities.device)\n",
    "\n",
    "    loss0 = nn.CrossEntropyLoss()(similarities, labels)  # labels: arange(batch_size)\n",
    "    loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
    "    return loss0, loss1, att_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_batch_size = 10\n",
    "num_cluster=8\n",
    "\n",
    "normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    normalize,\n",
    "        ])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "wsi_batch_dataset = WSIBatchedDataset(df, dtype='train', tokenizer=tokenizer, \\\n",
    "                                      img_transform=transform, pid_batch_size=pid_batch_size)\n",
    "train_dloader = torch.utils.data.DataLoader(wsi_batch_dataset,batch_size=1, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_val_dataset = WSIBatchedDataset(df, dtype='val', tokenizer=tokenizer, \\\n",
    "                                      img_transform=transform, pid_batch_size=pid_batch_size)\n",
    "val_dloader = torch.utils.data.DataLoader(wsi_val_dataset,batch_size=1, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538.26816\n"
     ]
    }
   ],
   "source": [
    "base_image_model = nn.DataParallel(ResnetPreTrained())\n",
    "base_image_model.to(device)\n",
    "bert_model = nn.DataParallel(BertEncoder(device=device))\n",
    "bert_model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_params(text_encoder):\n",
    "    freeze_modules = [text_encoder.module.model.embeddings, *text_encoder.module.model.encoder.layer[:-4]]\n",
    "    non_freeze_modules = [*text_encoder.module.model.encoder.layer[:-4]]\n",
    "    \n",
    "    param_list = []\n",
    "    for module in freeze_modules:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False\n",
    "            param_list.append(param)\n",
    "\n",
    "    for module in non_freeze_modules:\n",
    "        for param in module.parameters():\n",
    "            param_list.append(param)\n",
    "            \n",
    "    return param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_pretraining(img_encoder, text_encoder, train_loader, val_loader, device, pid_batch_size=pid_batch_size, epochs=50):\n",
    "    \n",
    "    params = list(img_encoder.parameters()) + get_bert_params(text_encoder)\n",
    "    optimizer = optim.Adadelta([param for param in params \\\n",
    "                                if param.requires_grad == True],lr=1e-3,rho=0.95)\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    best_val_loss=10\n",
    "    best_img_model=0\n",
    "    best_text_model=0\n",
    "    epochs_since_improvement = 0\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        \n",
    "        if epochs_since_improvement == 20:\n",
    "            break\n",
    "\n",
    "        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
    "            print(\"\\nDECAYING learning rate.\")\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = param_group['lr'] * 0.8\n",
    "            print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "        \n",
    "        img_encoder.train()\n",
    "        text_encoder.train()\n",
    "        \n",
    "        # Tracking time and loss\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (img, text, attention, token_typ, img_seps, pids) in enumerate(train_loader):\n",
    "\n",
    "            img_seps = [0]+[i.numpy()[0] for i in img_seps]\n",
    "            img_seps = [[img_seps[i],img_seps[i+1]] for i in range(len(img_seps)-1)]\n",
    "            \n",
    "            img, text = img.squeeze(0).to(device),text.squeeze(0).to(device)\n",
    "            attention, token_typ = attention.squeeze(0).to(device), token_typ.squeeze(0).to(device)\n",
    "            \n",
    "            text_outputs = text_encoder(text, attention, token_typ)\n",
    "            img_outputs = img_encoder(img)\n",
    "\n",
    "            # clean up\n",
    "            del img, text, attention, token_typ\n",
    "            gc.collect()\n",
    "\n",
    "            cap_lens = text_outputs[2]\n",
    "            cap_lens = [cap_lens[i].item() for i in np.arange(0, len(cap_lens), num_cluster)]\n",
    "\n",
    "            pid_word_embeddings = [text_outputs[0][x:y] for x, y in img_seps]\n",
    "            pid_sent_embeddings = [text_outputs[1][x:y] for x, y in img_seps]\n",
    "            pid_img_embeddings = [img_outputs[x:y] for x, y in img_seps]\n",
    "\n",
    "            cnn_code = torch.stack([x.mean(dim=0) for x in pid_img_embeddings])\n",
    "            rnn_code = torch.stack([x[0] for x in pid_sent_embeddings])\n",
    "\n",
    "            img_features = [x.unsqueeze(2).unsqueeze(2) for x in pid_img_embeddings]\n",
    "            img_features = [x.permute(2,1,0,3) for x in img_features]\n",
    "            img_features = torch.stack(img_features, dim=0).squeeze(1)\n",
    "\n",
    "            words_embs = [x[0] for x in pid_word_embeddings]\n",
    "            words_embs = torch.stack(words_embs)\n",
    "\n",
    "            gloss0, gloss1 = global_loss(cnn_code, rnn_code)\n",
    "            loss0, loss1, att_maps=local_loss(img_features, words_embs, cap_lens)\n",
    "            loss = loss0 + loss1 + 0.1*gloss0 + 0.1*gloss1\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if val_loader is not None:\n",
    "            \n",
    "            val_loss = evaluate(img_encoder, text_encoder, val_loader, device, test_dataloader=None)\n",
    "            \n",
    "            is_best = val_loss < best_val_loss\n",
    "            best_val_loss = min(val_loss, best_val_loss)\n",
    "            \n",
    "            if is_best:\n",
    "                best_img_model = img_encoder\n",
    "                best_text_model = text_encoder\n",
    "            \n",
    "            if not is_best:\n",
    "                epochs_since_improvement += 1\n",
    "            else:\n",
    "                epochs_since_improvement = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "    \n",
    "    return best_img_model, best_text_model\n",
    "\n",
    "def evaluate(img_encoder, text_encoder, val_dataloader, device, test_dataloader=None,plot=False):\n",
    "    img_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    \n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set..\n",
    "    for img, text, attention, token_typ, img_seps, pids in val_dataloader:\n",
    "        \n",
    "        img_seps = [0]+[i.numpy()[0] for i in img_seps]\n",
    "        img_seps = [[img_seps[i],img_seps[i+1]] for i in range(len(img_seps)-1)]\n",
    "\n",
    "        img, text = img.squeeze(0).to(device),text.squeeze(0).to(device)\n",
    "        attention, token_typ = attention.squeeze(0).to(device), token_typ.squeeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_outputs = text_encoder(text, attention, token_typ)\n",
    "            img_outputs = img_encoder(img)\n",
    "        \n",
    "        cap_lens = text_outputs[2]\n",
    "        cap_lens = [cap_lens[i].item() for i in np.arange(0, len(cap_lens), num_cluster)]\n",
    "        \n",
    "        pid_word_embeddings = [text_outputs[0][x:y] for x, y in img_seps]\n",
    "        pid_sent_embeddings = [text_outputs[1][x:y] for x, y in img_seps]\n",
    "        pid_img_embeddings = [img_outputs[x:y] for x, y in img_seps]\n",
    "\n",
    "        cnn_code = torch.stack([x.mean(dim=0) for x in pid_img_embeddings])\n",
    "        rnn_code = torch.stack([x[0] for x in pid_sent_embeddings])\n",
    "\n",
    "        img_features = [x.unsqueeze(2).unsqueeze(2) for x in pid_img_embeddings]\n",
    "        img_features = [x.permute(2,1,0,3) for x in img_features]\n",
    "        img_features = torch.stack(img_features, dim=0).squeeze(1)\n",
    "\n",
    "        words_embs = [x[0] for x in pid_word_embeddings]\n",
    "        words_embs = torch.stack(words_embs)\n",
    "\n",
    "        gloss0, gloss1 = global_loss(cnn_code, rnn_code)\n",
    "        loss0, loss1, att_maps=local_loss(img_features, words_embs, cap_lens)\n",
    "        loss = loss0 + loss1 + 0.1*gloss0 + 0.1*gloss1\n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "    val_loss = np.mean(val_loss)\n",
    "    \n",
    "    return val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   8.300355   |  8.542545  |   84.61  \n",
      "   2    |   7.058738   |  7.807192  |   83.50  \n",
      "   3    |   6.711402   |  7.614232  |   81.73  \n",
      "   4    |   6.558301   |  7.455561  |   80.20  \n",
      "   5    |   6.364923   |  7.319637  |   80.15  \n",
      "   6    |   6.292632   |  7.201505  |   79.69  \n",
      "   7    |   6.191592   |  7.183501  |   79.20  \n",
      "   8    |   6.075982   |  7.145250  |   78.56  \n",
      "   9    |   6.056406   |  7.279336  |   78.57  \n",
      "  10    |   6.013784   |  7.336645  |   77.71  \n",
      "  11    |   5.982965   |  7.313731  |   77.32  \n",
      "  12    |   5.932807   |  7.461548  |   77.35  \n",
      "  13    |   5.862575   |  7.313488  |   76.99  \n",
      "  14    |   5.927340   |  7.182718  |   77.03  \n",
      "  15    |   5.923119   |  7.149048  |   76.73  \n",
      "  16    |   5.968211   |  7.361364  |   76.74  \n",
      "\n",
      "DECAYING learning rate.\n",
      "The new learning rate is 0.000800\n",
      "\n",
      "  17    |   6.005700   |  7.469219  |   75.78  \n",
      "  18    |   5.982011   |  7.189733  |   76.02  \n",
      "  19    |   5.982247   |  7.362209  |   75.32  \n",
      "  20    |   6.042229   |  7.304606  |   75.14  \n",
      "  21    |   6.018026   |  7.274620  |   75.42  \n",
      "  22    |   6.072904   |  7.264961  |   75.33  \n",
      "  23    |   5.983925   |  7.388713  |   75.39  \n",
      "  24    |   6.204161   |  7.654109  |   74.45  \n",
      "\n",
      "DECAYING learning rate.\n",
      "The new learning rate is 0.000640\n",
      "\n",
      "  25    |   6.421223   |  7.693548  |   74.76  \n",
      "  26    |   6.498471   |  7.968634  |   74.80  \n",
      "  27    |   6.533062   |  7.794299  |   74.59  \n",
      "  28    |   6.460352   |  7.693519  |   74.51  \n"
     ]
    }
   ],
   "source": [
    "best_img_model, best_text_model = start_pretraining(base_image_model, bert_model, train_dloader,\\\n",
    "                                                    val_loader=val_dloader, device=device, \\\n",
    "                                                    pid_batch_size=pid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(best_img_model.state_dict(), './best_img_model.pth')\n",
    "# torch.save(best_text_model.state_dict(), './best_text_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_img_model.module.state_dict(), './best_img_model_dp.pth')\n",
    "torch.save(best_text_model.module.state_dict(), './best_text_model_dp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_img_model.module.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.10.0",
   "language": "python",
   "name": "pytorch-1.10.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
