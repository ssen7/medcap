{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer, AutoModel, BertModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from py_files.models import BertEncoder, ResnetPreTrained, ImageEncoder\n",
    "from py_files.datasets import WSIBatchedDataset, GetRepsDataset\n",
    "\n",
    "import faiss\n",
    "\n",
    "import gc\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "def check_cuda():\n",
    "    if torch.cuda.is_available():       \n",
    "        device = torch.device(\"cuda\")\n",
    "        device_count = torch.cuda.device_count()\n",
    "        print(f'There are {device_count} GPU(s) available.')\n",
    "        for i in range(device_count):\n",
    "            print('Device name:', torch.cuda.get_device_name(i))\n",
    "        return device\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "        return device\n",
    "    \n",
    "device=check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch_paths</th>\n",
       "      <th>pid</th>\n",
       "      <th>svs_paths</th>\n",
       "      <th>dtype</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         patch_paths             pid  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "\n",
       "                                           svs_paths  dtype  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "\n",
       "                                               notes  \n",
       "0  2 pieces ~9.5x7 mm; 1 broken apart; good morph...  \n",
       "1  2 pieces ~9.5x7 mm; 1 broken apart; good morph...  \n",
       "2  2 pieces ~9.5x7 mm; 1 broken apart; good morph...  \n",
       "3  2 pieces ~9.5x7 mm; 1 broken apart; good morph...  \n",
       "4  2 pieces ~9.5x7 mm; 1 broken apart; good morph...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train global KMeans clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pids = df.pid.unique()\n",
    "len(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_percent_cluster=0.1\n",
    "n_pids_cluster = int(pid_percent_cluster*len(pids))\n",
    "n_pids_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6302"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pids_cluster = np.random.choice(pids, size=n_pids_cluster)\n",
    "\n",
    "normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    normalize,\n",
    "        ])\n",
    "\n",
    "cluster_dataset = GetRepsDataset(df, pids_cluster, transform)\n",
    "cluster_loader = torch.utils.data.DataLoader(cluster_dataset,batch_size=64, shuffle=True, \\\n",
    "                                             num_workers=1, pin_memory=True)\n",
    "len(cluster_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.279168\n"
     ]
    }
   ],
   "source": [
    "base_image_model = ResnetPreTrained()\n",
    "image_encoder = nn.DataParallel(ImageEncoder(base_image_model))\n",
    "image_encoder.to(device)\n",
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global_cluster_model(encoder, dataloader, device, ncentroids=8):\n",
    "    \n",
    "    print(\"Getting patch representations...\")\n",
    "    \n",
    "    rep_list = []\n",
    "    path_list = []\n",
    "    \n",
    "    encoder.eval()\n",
    "    \n",
    "    for img, path in tqdm(dataloader):\n",
    "\n",
    "        img.to(device)    \n",
    "\n",
    "        in_batch_size = img.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reps = encoder(img)\n",
    "        rep_list.append(reps.detach().detach().cpu().numpy().reshape(in_batch_size, -1))\n",
    "        path_list += path\n",
    "        \n",
    "        # clean up\n",
    "        del img\n",
    "        del reps\n",
    "    \n",
    "    print(\"\\nTraining KMeans model...\")\n",
    "    \n",
    "    X = np.concatenate(rep_list)\n",
    "    X = np.ascontiguousarray(X)\n",
    "    X = X.astype('float32')\n",
    "    \n",
    "    ncentroids = ncentroids\n",
    "    niter = 300\n",
    "    verbose = False\n",
    "    d = X.shape[1]\n",
    "    kmeans = faiss.Kmeans(d, ncentroids, niter=niter, verbose=verbose, nredo=20)\n",
    "    kmeans.train(X)\n",
    "    \n",
    "    print(\"\\nFinished training KMeans model...\")\n",
    "    \n",
    "    # clean up\n",
    "    del encoder\n",
    "    del dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting patch representations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a60229b9df46559852f433ce5968ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "Training KMeans model...\n",
      "\n",
      "Finished training KMeans model...\n"
     ]
    }
   ],
   "source": [
    "kmeans = train_global_cluster_model(image_encoder, cluster_loader, device, ncentroids=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.279168\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://forums.fast.ai/t/gpu-memory-not-being-freed-after-training-is-over/10265/8?u=cedric\n",
    "# def pretty_size(size):\n",
    "# \t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "# \tassert(isinstance(size, torch.Size))\n",
    "# \treturn \" × \".join(map(str, size))\n",
    "\n",
    "# def dump_tensors(gpu_only=True):\n",
    "# \t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "# \timport gc\n",
    "# \ttotal_size = 0\n",
    "# \tfor obj in gc.get_objects():\n",
    "# \t\ttry:\n",
    "# \t\t\tif torch.is_tensor(obj):\n",
    "# \t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "# \t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "# \t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "# \t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "# \t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "# \t\t\t\t\ttotal_size += obj.numel()\n",
    "# \t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "# \t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "# \t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "# \t\t\t\t\ttotal_size += obj.data.numel()\n",
    "# \t\texcept Exception as e:\n",
    "# \t\t\tpass        \n",
    "# \tprint(\"Total size:\", total_size*1e-6)\n",
    "\n",
    "# dump_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster all patches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69757"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcluster_dataset = GetRepsDataset(df, pids, transform)\n",
    "gcluster_loader = torch.utils.data.DataLoader(gcluster_dataset,batch_size=64, shuffle=False, \\\n",
    "                                             num_workers=1, pin_memory=True)\n",
    "len(gcluster_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.279168\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_all_patches(encoder, kmeans, dataloader, device, ncentroids=8):\n",
    "    \n",
    "    print(\"Clustering all patches...\")\n",
    "    \n",
    "    encoder.eval()\n",
    "    \n",
    "    path_list, rep_list = [], []\n",
    "    \n",
    "    for img, path in tqdm(dataloader):\n",
    "        img.to(device)\n",
    "        \n",
    "        in_batch_size = img.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reps = image_encoder(img)\n",
    "        rep_list.append(reps.detach().detach().cpu().numpy().reshape(in_batch_size, -1))\n",
    "        path_list += path\n",
    "        \n",
    "        # clean up\n",
    "        del img\n",
    "        del reps\n",
    "        \n",
    "    X = np.concatenate(rep_list)\n",
    "    X = np.ascontiguousarray(X)\n",
    "    X = X.astype('float32')\n",
    "    \n",
    "    D, I = kmeans.index.search(X, 1)\n",
    "    \n",
    "    df = pd.DataFrame(path_list, columns=['patch_paths'])\n",
    "    df['cluster_assignment'] = I\n",
    "    \n",
    "    print(\"\\nFinished clustering all patches...\")\n",
    "    \n",
    "    # clean up\n",
    "    del encoder\n",
    "    # del dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering all patches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564749fb1c0a49ac81bd4e3ee928dcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "Finished clustering all patches...\n"
     ]
    }
   ],
   "source": [
    "cluster_df = cluster_all_patches(image_encoder, kmeans, gcluster_loader, device, ncentroids=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.279168\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered = df.merge(cluster_df, on='patch_paths')\n",
    "\n",
    "df_clustered.to_csv('../df_clustered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538.26816\n"
     ]
    }
   ],
   "source": [
    "# del image_encoder\n",
    "# del cluster_loader\n",
    "# del cluster_dataset\n",
    "# del gcluster_loader\n",
    "# del gcluster_dataset\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoint - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch_paths</th>\n",
       "      <th>pid</th>\n",
       "      <th>svs_paths</th>\n",
       "      <th>dtype</th>\n",
       "      <th>notes</th>\n",
       "      <th>cluster_assignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pr...</td>\n",
       "      <td>GTEX-R55E-1726</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>train</td>\n",
       "      <td>2 pieces ~9.5x7 mm; 1 broken apart; good morph...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         patch_paths             pid  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/pr...  GTEX-R55E-1726   \n",
       "\n",
       "                                           svs_paths  dtype  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...  train   \n",
       "\n",
       "                                               notes  cluster_assignment  \n",
       "0  2 pieces ~9.5x7 mm; 1 broken apart; good morph...                   6  \n",
       "1  2 pieces ~9.5x7 mm; 1 broken apart; good morph...                   6  \n",
       "2  2 pieces ~9.5x7 mm; 1 broken apart; good morph...                   6  \n",
       "3  2 pieces ~9.5x7 mm; 1 broken apart; good morph...                   6  \n",
       "4  2 pieces ~9.5x7 mm; 1 broken apart; good morph...                   6  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../df_clustered.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_loss(cnn_code, rnn_code, eps=1e-8, temp3=10.0):\n",
    "\n",
    "    batch_size = cnn_code.shape[0]\n",
    "    labels = Variable(torch.LongTensor(range(batch_size))).to(cnn_code.device)\n",
    "\n",
    "    if cnn_code.dim() == 2 :\n",
    "        cnn_code = cnn_code.unsqueeze(0)\n",
    "        rnn_code = rnn_code.unsqueeze(0)\n",
    "        \n",
    "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
    "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
    "\n",
    "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1,2))\n",
    "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
    "    scores0 = scores0 / norm0.clamp(min=eps) * temp3\n",
    "    \n",
    "    # --> batch_size x batch_size\n",
    "    if scores0.shape[0]!=1:\n",
    "        scores0 = scores0.squeeze()\n",
    "    else:\n",
    "        scores0 = scores0.squeeze(0)\n",
    "    \n",
    "\n",
    "    scores1 = scores0.transpose(0, 1)\n",
    "    loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
    "    loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
    "    return loss0, loss1\n",
    "\n",
    "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n",
    "\n",
    "def attention_fn(query, context, temp1):\n",
    "    \"\"\"\n",
    "    query: batch x ndf x queryL\n",
    "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
    "    mask: batch_size x sourceL\n",
    "    \"\"\"\n",
    "    batch_size, queryL = query.size(0), query.size(2)\n",
    "    ih, iw = context.size(2), context.size(3)\n",
    "    sourceL = ih * iw\n",
    "\n",
    "    # --> batch x sourceL x ndf\n",
    "    context = context.view(batch_size, -1, sourceL)\n",
    "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
    "\n",
    "    # Get attention\n",
    "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
    "    # -->batch x sourceL x queryL\n",
    "    attn = torch.bmm(contextT, query)\n",
    "    # --> batch*sourceL x queryL\n",
    "    attn = attn.view(batch_size * sourceL, queryL)\n",
    "    attn = nn.Softmax(dim=-1)(attn)\n",
    "\n",
    "    # --> batch x sourceL x queryL\n",
    "    attn = attn.view(batch_size, sourceL, queryL)\n",
    "    # --> batch*queryL x sourceL\n",
    "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "    attn = attn.view(batch_size * queryL, sourceL)\n",
    "\n",
    "    attn = attn * temp1\n",
    "    attn = nn.Softmax(dim=-1)(attn)\n",
    "    attn = attn.view(batch_size, queryL, sourceL)\n",
    "    # --> batch x sourceL x queryL\n",
    "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
    "    # --> batch x ndf x queryL\n",
    "    weightedContext = torch.bmm(context, attnT)\n",
    "\n",
    "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "def local_loss(\n",
    "    img_features, words_emb, cap_lens, temp1=4.0, temp2=5.0, temp3=10.0, agg=\"sum\"\n",
    "):\n",
    "\n",
    "    batch_size = img_features.shape[0]\n",
    "\n",
    "    att_maps = []\n",
    "    similarities = []\n",
    "    # cap_lens = cap_lens.data.tolist()\n",
    "    for i in range(words_emb.shape[0]):\n",
    "\n",
    "        # Get the i-th text description\n",
    "        words_num = cap_lens[i]  # 25\n",
    "        # TODO: remove [SEP]\n",
    "        # word = words_emb[i, :, 1:words_num+1].unsqueeze(0).contiguous()    # [1, 768, 25]\n",
    "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()  # [1, 768, 25]\n",
    "        word = word.repeat(batch_size, 1, 1)  # [48, 768, 25]\n",
    "        context = img_features  # [48, 768, 19, 19]\n",
    "\n",
    "        weiContext, attn = attention_fn(\n",
    "            word, context, temp1\n",
    "        )  # [48, 768, 25], [48, 25, 19, 19]\n",
    "\n",
    "        att_maps.append(\n",
    "            attn[i].unsqueeze(0).contiguous()\n",
    "        )  # add attention for curr index  [25, 19, 19]\n",
    "        word = word.transpose(1, 2).contiguous()  # [48, 25, 768]\n",
    "        weiContext = weiContext.transpose(1, 2).contiguous()  # [48, 25, 768]\n",
    "\n",
    "        word = word.view(batch_size * words_num, -1)  # [1200, 768]\n",
    "        weiContext = weiContext.view(batch_size * words_num, -1)  # [1200, 768]\n",
    "\n",
    "        row_sim = cosine_similarity(word, weiContext)\n",
    "        row_sim = row_sim.view(batch_size, words_num)  # [48, 25]\n",
    "\n",
    "        row_sim.mul_(temp2).exp_()\n",
    "        if agg == \"sum\":\n",
    "            row_sim = row_sim.sum(dim=1, keepdim=True)  # [48, 1]\n",
    "        else:\n",
    "            row_sim = row_sim.mean(dim=1, keepdim=True)  # [48, 1]\n",
    "        row_sim = torch.log(row_sim)\n",
    "\n",
    "        similarities.append(row_sim)\n",
    "\n",
    "    similarities = torch.cat(similarities, 1)  #\n",
    "    similarities = similarities * temp3\n",
    "    similarities1 = similarities.transpose(0, 1)  # [48, 48]\n",
    "\n",
    "    labels = Variable(torch.LongTensor(range(batch_size))).to(similarities.device)\n",
    "\n",
    "    loss0 = nn.CrossEntropyLoss()(similarities, labels)  # labels: arange(batch_size)\n",
    "    loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
    "    return loss0, loss1, att_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_batch_size = 8\n",
    "\n",
    "normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    normalize,\n",
    "        ])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "wsi_batch_dataset = WSIBatchedDataset(df, dtype='train', tokenizer=tokenizer, \\\n",
    "                                      img_transform=transform, pid_batch_size=pid_batch_size)\n",
    "train_dloader = torch.utils.data.DataLoader(wsi_batch_dataset,batch_size=1, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.279168\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634.120192\n"
     ]
    }
   ],
   "source": [
    "base_image_model = nn.DataParallel(ResnetPreTrained())\n",
    "base_image_model.to(device)\n",
    "bert_model = nn.DataParallel(BertEncoder())\n",
    "bert_model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train_encoder(img_encoder, text_encoder, train_loader, val_loader, device, \\\n",
    "                      loss_fn=None, optimizer=None, lr_scheduler=None, count=6, pid_batch_size=8):\n",
    "    \n",
    "    img_encoder.train()\n",
    "    text_encoder.train()\n",
    "    \n",
    "    img_optimizer = optim.Adadelta([param for param in img_encoder.parameters() \\\n",
    "                                if param.requires_grad == True],lr=1e-3,rho=0.95)\n",
    "    \n",
    "    for i, (img, text, attention, token_typ, img_seps, pids) in enumerate(train_loader):\n",
    "        \n",
    "        img_seps = [0]+[i.numpy()[0] for i in img_seps]\n",
    "        img_seps = [[img_seps[i],img_seps[i+1]] for i in range(len(img_seps)-1)]\n",
    "        \n",
    "        img, text = img.squeeze(0),text.squeeze(0)\n",
    "        attention, token_typ = attention.squeeze(0), token_typ.squeeze(0)\n",
    "\n",
    "        img.to(device)\n",
    "        text.to(device)\n",
    "        attention.to(device)\n",
    "        token_typ.to(device)\n",
    "\n",
    "        text_outputs = bert_model(text, attention, token_typ)\n",
    "        img_outputs = base_image_model(img)\n",
    "\n",
    "        cap_lens = [len([w for w in sent if not w.startswith(\"[\")]) + 1 for sent in text_outputs[2]]\n",
    "        cap_lens = [cap_lens[i] for i in np.arange(0, len(cap_lens), pid_batch_size)]\n",
    "        \n",
    "        pid_word_embeddings = [text_outputs[0][x:y] for x, y in img_seps]\n",
    "        pid_sent_embeddings = [text_outputs[1][x:y] for x, y in img_seps]\n",
    "        pid_img_embeddings = [img_outputs[x:y] for x, y in img_seps]\n",
    "        \n",
    "        del img_outputs, text_outputs\n",
    "        gc.collect()\n",
    "        \n",
    "        cnn_code = torch.stack([x.mean(dim=0) for x in pid_img_embeddings])\n",
    "        rnn_code = torch.stack([x[0] for x in pid_sent_embeddings])\n",
    "        \n",
    "        img_features = [x.unsqueeze(2).unsqueeze(2) for x in pid_img_embeddings]\n",
    "        img_features = [x.permute(2,1,0,3) for x in img_features]\n",
    "        img_features = torch.stack(img_features, dim=0).squeeze(1)\n",
    "        \n",
    "        words_embs = [x[0] for x in pid_word_embeddings]\n",
    "        words_embs = torch.stack(words_embs)\n",
    "        \n",
    "        gloss0, gloss1 = global_loss(cnn_code, rnn_code)\n",
    "        \n",
    "        \n",
    "        loss0, loss1, att_maps=local_loss(img_features, words_embs, cap_lens)\n",
    "        \n",
    "        total_loss = loss0 + loss1 + 0.1*gloss0 + 0.1*gloss1\n",
    "        \n",
    "        total_loss.backward()\n",
    "        img_optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # clean up\n",
    "        del img, text, attention, token_typ, cnn_code, rnn_code, words_embs, img_features\n",
    "        gc.collect()\n",
    "        \n",
    "        count-=1\n",
    "        \n",
    "        print(count)\n",
    "        torch.cuda.empty_cache()\n",
    "        print(torch.cuda.memory_allocated(device)*1e-6)\n",
    "        if count==0:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    \n",
    "    return img_encoder\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "5\n",
      "1394.2615039999998\n",
      "4\n",
      "1394.2599679999998\n",
      "3\n",
      "1394.2599679999998\n",
      "2\n",
      "1394.250752\n",
      "1\n",
      "1394.259456\n",
      "0\n",
      "1394.254848\n"
     ]
    }
   ],
   "source": [
    "img_encoder = pre_train_encoder(base_image_model, bert_model, train_dloader, val_loader=None, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1175.3533439999999\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.10.0",
   "language": "python",
   "name": "pytorch-1.10.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
